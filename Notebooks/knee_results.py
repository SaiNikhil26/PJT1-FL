# -*- coding: utf-8 -*-
"""Knee-Results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CB3aOl32x0Fq1x1kNnoMdRPKMobozrYz

# **Extracting Dataset**
"""

import os
import zipfile

zip_path = "/content/Dropped-Dataset.zip"
extract_path = "/content/Dataset2"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

"""# **Importing Dependencies**"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import DenseNet121, DenseNet201, EfficientNetV2L, InceptionResNetV2, DenseNet169
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
from typing import List, Tuple, Dict
from sklearn.utils.class_weight import compute_class_weight
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

"""# **Constants**"""

# Constants
NUM_CLIENTS = 3
NUM_ROUNDS = 12
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
NUM_CLASSES = 3
CLASS_NAMES = ['Healthy', 'Moderate', 'Severe']
BASE_DIR = "/content/Dataset2/archive"

"""# **Client Data Class**"""

class ClientData:
    def __init__(self, generator, num_samples, class_weights):
        self.generator = generator
        self.num_samples = num_samples
        self.class_weights = class_weights

"""# **Load Image Files from each directory**"""

def get_class_dirs(base_path):
    """Get all image files from each class directory."""
    class_dirs = {}
    for class_name in CLASS_NAMES:
        class_path = os.path.join(base_path, 'train', class_name)
        image_files = [os.path.join(class_name, f) for f in os.listdir(class_path)
                      if f.endswith(('.png', '.jpg', '.jpeg'))]
        class_dirs[class_name] = image_files
    return class_dirs

"""# **Client Shards of data**"""

def create_client_shards():
    """Create client shards by dividing data among clients."""
    class_dirs = get_class_dirs(BASE_DIR)
    client_data = [[] for _ in range(NUM_CLIENTS)]
    client_sample_counts = [0] * NUM_CLIENTS

    # Distribute files from each class evenly among clients
    for class_name, files in class_dirs.items():
        np.random.shuffle(files)
        files_per_client = len(files) // NUM_CLIENTS

        for client_idx in range(NUM_CLIENTS):
            start_idx = client_idx * files_per_client
            end_idx = start_idx + files_per_client if client_idx < NUM_CLIENTS - 1 else len(files)
            client_files = files[start_idx:end_idx]
            client_data[client_idx].extend(client_files)
            client_sample_counts[client_idx] += len(client_files)

    return client_data, client_sample_counts

"""# **Data Generators**"""

def create_data_generator(is_training=True):
    """Create data generator with appropriate augmentation."""
    if is_training:
        return ImageDataGenerator(
            rescale=1./255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest'
        )
    return ImageDataGenerator(rescale=1./255)

"""# **Class Weights**"""

def compute_class_weights(client_data):
    """Compute class weights for balancing the classes."""
    all_labels = []
    for shard in client_data:
        for class_name in CLASS_NAMES:
            all_labels.extend([class_name] * len(shard))

    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.unique(all_labels),
        y=all_labels
    )
    return {i: class_weights[i] for i in range(NUM_CLASSES)}

"""# **Client Generators**"""

def create_client_generators(client_shards, client_sample_counts):
    """Create data generators for each client's shard."""
    train_datagen = create_data_generator(is_training=True)
    client_generators = []

    for client_idx, (shard, num_samples) in enumerate(zip(client_shards, client_sample_counts)):
        # Create generator for this client
        generator = train_datagen.flow_from_directory(
            os.path.join(BASE_DIR, 'train'),
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            shuffle=True,
            classes=CLASS_NAMES
        )

        class_weights = compute_class_weights(shard)
        client_generators.append(ClientData(generator, num_samples, class_weights))

    return client_generators

"""# **Validation Test and AutoTest Generators**"""

def load_validation_test_data():
    """Load validation and test data."""
    val_datagen = create_data_generator(is_training=False)

    val_generator = val_datagen.flow_from_directory(
        os.path.join(BASE_DIR, 'val'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )

    test_generator = val_datagen.flow_from_directory(
        os.path.join(BASE_DIR, 'test'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )

    autotest_generator = val_datagen.flow_from_directory(
        os.path.join(BASE_DIR,'auto_test'),
        target_size = IMG_SIZE,
        batch_size = BATCH_SIZE,
        class_mode = 'categorical',
        shuffle = False
    )

    return val_generator, test_generator, autotest_generator

"""# **Inception-ResNet Model**"""

def create_inception_resnet_model():
    """Create and compile the InceptionResNetV2 model."""
    base_model = InceptionResNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(*IMG_SIZE, 3)
    )

    base_model.trainable = False

    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

"""# **DenseNet201 Model**"""

def create_densenet201_model():
    """Create and compile the DenseNet201 Model."""
    base_model = DenseNet201(
        weights='imagenet',
        include_top=False,
        input_shape=(*IMG_SIZE, 3)
    )

    base_model.trainable = False

    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

"""# **DenseNet169 Model**"""

def create_densenet169_model():
    """Create and compile the DenseNet169 model."""
    base_model = DenseNet169(
        weights='imagenet',
        include_top=False,
        input_shape=(*IMG_SIZE, 3)
    )

    base_model.trainable = False

    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

"""# **Client Training**"""

def client_update(model, client_data: ClientData):
    """Train the model on client data."""
    steps = client_data.num_samples // BATCH_SIZE

    history = model.fit(
        client_data.generator,
        steps_per_epoch=steps,
        epochs=1,
        verbose=0,
        class_weight=client_data.class_weights
    )
    return model.get_weights(), history.history['accuracy'][0]

"""# **FedAverage Algorithm**"""

def weighted_average_weights(weights_list: List[np.ndarray],
                           sample_counts: List[int]) -> List[np.ndarray]:
    """Compute weighted average of model weights based on number of samples."""
    total_samples = sum(sample_counts)
    weight_scaling_factors = [count / total_samples for count in sample_counts]

    avg_weights = []
    for layer_weights in zip(*weights_list):
        layer_weights = np.array(layer_weights)
        scaled_weights = np.array([w * scale for w, scale in zip(layer_weights, weight_scaling_factors)])
        avg_weights.append(np.sum(scaled_weights, axis=0))

    return avg_weights

"""# **Main Training**

**1. DenseNet201 Model**
"""

MODEL_NAME = "DenseNet201"  # Set your model name here
print("DenseNet201 Model")
print("-" * 50)

def main():
    print("Creating client shards...")
    client_shards, client_sample_counts = create_client_shards()

    print("Creating data generators...")
    client_data_list = create_client_generators(client_shards, client_sample_counts)
    val_generator, test_generator, autotest_generator = load_validation_test_data()

    print("\nClient data distribution:")
    total_samples = sum(client_sample_counts)
    for i, count in enumerate(client_sample_counts):
        print(f"Client {i + 1}: {count} samples ({count / total_samples * 100:.1f}%)")

    print("\nCreating global model...")
    global_model = create_densenet201_model()

    print("\nStarting federated learning...")
    print("-" * 50)

    client_accuracies_history = []
    global_accuracies_history = []

    # DataFrame to store accuracies
    accuracies_df = pd.DataFrame(columns=['Round', 'Client', 'Client Accuracy', 'Global Accuracy'])

    # Federated Learning Loop
    for round_num in range(NUM_ROUNDS):
        print(f"\nRound {round_num + 1}/{NUM_ROUNDS}")
        print("-" * 30)

        client_weights = []
        round_client_accuracies = []

        # Train each client
        for client_idx, client_data in enumerate(client_data_list):
            print(f"Training Client {client_idx + 1}...")

            # Initialize client model with global weights
            client_model = create_densenet201_model()
            client_model.set_weights(global_model.get_weights())

            # Train on client's shard
            updated_weights, client_accuracy = client_update(client_model, client_data)

            client_weights.append(updated_weights)
            round_client_accuracies.append(client_accuracy)

            # Append to DataFrame
            new_row = pd.DataFrame({
                'Round': [round_num + 1],
                'Client': [client_idx + 1],
                'Client Accuracy': [client_accuracy]
            })
            accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)

            print(f"Client {client_idx + 1} Accuracy: {client_accuracy:.4f}")

        # Store metrics
        client_accuracies_history.append(round_client_accuracies)

        # Compute weighted average of client weights
        global_weights = weighted_average_weights(client_weights, client_sample_counts)
        global_model.set_weights(global_weights)

        # Evaluate global model
        global_metrics = global_model.evaluate(val_generator, verbose=0)
        global_accuracies_history.append(global_metrics[1])

        # Append global accuracy to DataFrame
        accuracies_df.loc[accuracies_df['Round'] == (round_num + 1), 'Global Accuracy'] = global_metrics[1]

        print("\nRound Summary:")
        print(f"Average Client Accuracy: {np.mean(round_client_accuracies):.4f}")
        print(f"Global Model Accuracy: {global_metrics[1]:.4f}")
        print(f"Global Model Loss: {global_metrics[0]:.4f}")
        print("-" * 30)

    # Save the global model
    global_model.save(f"{MODEL_NAME}.h5")

    # Final evaluation
    final_metrics = global_model.evaluate(test_generator, verbose=0)
    print("\nFinal Test Results:")
    print("-" * 20)
    print(f"Test Loss: {final_metrics[0]:.4f}")
    print(f"Test Accuracy: {final_metrics[1]:.4f}")

    # Autotest Evaluation
    autotest_metrics = global_model.evaluate(autotest_generator,verbose=0)
    print("\nAutoTest Results:")
    print("-" * 20)
    print(f"Test Loss: {autotest_metrics[0]:.4f}")
    print(f"Test Accuracy: {autotest_metrics[1]:.4f}")

    # Print test dataset classification report
    y_true = test_generator.classes
    y_pred = np.argmax(global_model.predict(test_generator), axis=1)
    print("\nTest dataset - Classification Report:")
    print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))

    # Print autotest dataset classification report
    y_true_auto = autotest_generator.classes
    y_pred_auto = np.argmax(global_model.predict(autotest_generator),axis=1)
    print("\nAutotest dataset - Classification Report:")
    print(classification_report(y_true_auto, y_pred_auto, target_names=autotest_generator.class_indices.keys()))

    # Plot Test dataset confusion matrix
    print("\nTest dataset - Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=test_generator.class_indices.keys(),
                yticklabels=test_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Test Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_test_confusion_matrix.png")
    plt.show()

    # Plot autotest dataset - confusion matrix
    print("\nAutotest dataset - Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true_auto, y_pred_auto)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=autotest_generator.class_indices.keys(),
                yticklabels=autotest_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Autotest Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_autotest_confusion_matrix.png")
    plt.show()


    # Plot global model accuracy and loss for each round
    rounds = np.arange(1, NUM_ROUNDS + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(rounds, global_accuracies_history, marker='o', label='Global Accuracy')
    plt.title(f'{MODEL_NAME} Global Model Accuracy')
    plt.xlabel('Rounds')
    plt.ylabel('Accuracy')
    plt.xticks(rounds)
    plt.ylim([0, 1])
    plt.grid()
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(rounds, [global_metrics[0]] * NUM_ROUNDS, marker='o', label='Global Loss')
    plt.title(f'{MODEL_NAME} Global Model Loss')
    plt.xlabel('Rounds')
    plt.ylabel('Loss')
    plt.xticks(rounds)
    plt.grid()
    plt.legend()

    plt.tight_layout()
    plt.savefig(f"{MODEL_NAME}_global_accuracy_loss.png")
    plt.show()

    # Save accuracies DataFrame to a CSV file
    accuracies_df.to_csv(f"{MODEL_NAME}_accuracies.csv", index=False)

if __name__ == "__main__":
    main()

"""**2. DenseNet169 Model**"""

MODEL_NAME = "DenseNet169"  # Set your model name here
print("DenseNet169 Model")
print("-" * 50)

def main():
    print("Creating client shards...")
    client_shards, client_sample_counts = create_client_shards()

    print("Creating data generators...")
    client_data_list = create_client_generators(client_shards, client_sample_counts)
    val_generator, test_generator,autotest_generator = load_validation_test_data()

    print("\nClient data distribution:")
    total_samples = sum(client_sample_counts)
    for i, count in enumerate(client_sample_counts):
        print(f"Client {i + 1}: {count} samples ({count / total_samples * 100:.1f}%)")

    print("\nCreating global model...")
    global_model = create_densenet169_model()

    print("\nStarting federated learning...")
    print("-" * 50)

    client_accuracies_history = []
    global_accuracies_history = []

    # DataFrame to store accuracies
    accuracies_df = pd.DataFrame(columns=['Round', 'Client', 'Client Accuracy', 'Global Accuracy'])

    # Federated Learning Loop
    for round_num in range(NUM_ROUNDS):
        print(f"\nRound {round_num + 1}/{NUM_ROUNDS}")
        print("-" * 30)

        client_weights = []
        round_client_accuracies = []

        # Train each client
        for client_idx, client_data in enumerate(client_data_list):
            print(f"Training Client {client_idx + 1}...")

            # Initialize client model with global weights
            client_model = create_densenet169_model()
            client_model.set_weights(global_model.get_weights())

            # Train on client's shard
            updated_weights, client_accuracy = client_update(client_model, client_data)

            client_weights.append(updated_weights)
            round_client_accuracies.append(client_accuracy)

            # Append to DataFrame
            new_row = pd.DataFrame({
                'Round': [round_num + 1],
                'Client': [client_idx + 1],
                'Client Accuracy': [client_accuracy]
            })
            accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)

            print(f"Client {client_idx + 1} Accuracy: {client_accuracy:.4f}")

        # Store metrics
        client_accuracies_history.append(round_client_accuracies)

        # Compute weighted average of client weights
        global_weights = weighted_average_weights(client_weights, client_sample_counts)
        global_model.set_weights(global_weights)

        # Evaluate global model
        global_metrics = global_model.evaluate(val_generator, verbose=0)
        global_accuracies_history.append(global_metrics[1])

        # Append global accuracy to DataFrame
        accuracies_df.loc[accuracies_df['Round'] == (round_num + 1), 'Global Accuracy'] = global_metrics[1]

        print("\nRound Summary:")
        print(f"Average Client Accuracy: {np.mean(round_client_accuracies):.4f}")
        print(f"Global Model Accuracy: {global_metrics[1]:.4f}")
        print(f"Global Model Loss: {global_metrics[0]:.4f}")
        print("-" * 30)

    # Save the global model
    global_model.save(f"{MODEL_NAME}.h5")

    # Final evaluation
    final_metrics = global_model.evaluate(test_generator, verbose=0)
    print("\nFinal Test Results:")
    print("-" * 20)
    print(f"Test Loss: {final_metrics[0]:.4f}")
    print(f"Test Accuracy: {final_metrics[1]:.4f}")

    # Autotest Evaluation
    autotest_metrics = global_model.evaluate(autotest_generator,verbose=0)
    print("\nAutoTest Results:")
    print("-" * 20)
    print(f"Test Loss: {autotest_metrics[0]:.4f}")
    print(f"Test Accuracy: {autotest_metrics[1]:.4f}")

    # Print test dataset classification report
    y_true = test_generator.classes
    y_pred = np.argmax(global_model.predict(test_generator), axis=1)
    print("\nTest dataset - Classification Report:")
    print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))

    # Print autotest dataset classification report
    y_true_auto = autotest_generator.classes
    y_pred_auto = np.argmax(global_model.predict(autotest_generator),axis=1)
    print("\nAutotest dataset - Classification Report:")
    print(classification_report(y_true_auto, y_pred_auto, target_names=autotest_generator.class_indices.keys()))

    # Plot confusion matrix
    print("\nTest Dataset Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=test_generator.class_indices.keys(),
                yticklabels=test_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Test Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_test_confusion_matrix.png")
    plt.show()

    # Plot autotest dataset - confusion matrix
    print("\nAutotest dataset - Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true_auto, y_pred_auto)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=autotest_generator.class_indices.keys(),
                yticklabels=autotest_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Autotest Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_autotest_confusion_matrix.png")
    plt.show()


    # Plot global model accuracy and loss for each round
    rounds = np.arange(1, NUM_ROUNDS + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(rounds, global_accuracies_history, marker='o', label='Global Accuracy')
    plt.title(f'{MODEL_NAME} Global Model Accuracy')
    plt.xlabel('Rounds')
    plt.ylabel('Accuracy')
    plt.xticks(rounds)
    plt.ylim([0, 1])
    plt.grid()
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(rounds, [global_metrics[0]] * NUM_ROUNDS, marker='o', label='Global Loss')
    plt.title(f'{MODEL_NAME} Global Model Loss')
    plt.xlabel('Rounds')
    plt.ylabel('Loss')
    plt.xticks(rounds)
    plt.grid()
    plt.legend()

    plt.tight_layout()
    plt.savefig(f"{MODEL_NAME}_global_accuracy_loss.png")
    plt.show()

    # Save accuracies DataFrame to a CSV file
    accuracies_df.to_csv(f"{MODEL_NAME}_accuracies.csv", index=False)

if __name__ == "__main__":
    main()

"""**3. Inception-ResNet V2 Model**"""

MODEL_NAME = "InceptionResNet"  # Set your model name here
print("InceptionResNet Model")
print("-" * 50)

def main():
    print("Creating client shards...")
    client_shards, client_sample_counts = create_client_shards()

    print("Creating data generators...")
    client_data_list = create_client_generators(client_shards, client_sample_counts)
    val_generator, test_generator,autotest_generator = load_validation_test_data()

    print("\nClient data distribution:")
    total_samples = sum(client_sample_counts)
    for i, count in enumerate(client_sample_counts):
        print(f"Client {i + 1}: {count} samples ({count / total_samples * 100:.1f}%)")

    print("\nCreating global model...")
    global_model = create_inception_resnet_model()

    print("\nStarting federated learning...")
    print("-" * 50)

    client_accuracies_history = []
    global_accuracies_history = []

    # DataFrame to store accuracies
    accuracies_df = pd.DataFrame(columns=['Round', 'Client', 'Client Accuracy', 'Global Accuracy'])

    # Federated Learning Loop
    for round_num in range(NUM_ROUNDS):
        print(f"\nRound {round_num + 1}/{NUM_ROUNDS}")
        print("-" * 30)

        client_weights = []
        round_client_accuracies = []

        # Train each client
        for client_idx, client_data in enumerate(client_data_list):
            print(f"Training Client {client_idx + 1}...")

            # Initialize client model with global weights
            client_model = create_inception_resnet_model()
            client_model.set_weights(global_model.get_weights())

            # Train on client's shard
            updated_weights, client_accuracy = client_update(client_model, client_data)

            client_weights.append(updated_weights)
            round_client_accuracies.append(client_accuracy)

            # Append to DataFrame
            new_row = pd.DataFrame({
                'Round': [round_num + 1],
                'Client': [client_idx + 1],
                'Client Accuracy': [client_accuracy]
            })
            accuracies_df = pd.concat([accuracies_df, new_row], ignore_index=True)

            print(f"Client {client_idx + 1} Accuracy: {client_accuracy:.4f}")

        # Store metrics
        client_accuracies_history.append(round_client_accuracies)

        # Compute weighted average of client weights
        global_weights = weighted_average_weights(client_weights, client_sample_counts)
        global_model.set_weights(global_weights)

        # Evaluate global model
        global_metrics = global_model.evaluate(val_generator, verbose=0)
        global_accuracies_history.append(global_metrics[1])

        # Append global accuracy to DataFrame
        accuracies_df.loc[accuracies_df['Round'] == (round_num + 1), 'Global Accuracy'] = global_metrics[1]

        print("\nRound Summary:")
        print(f"Average Client Accuracy: {np.mean(round_client_accuracies):.4f}")
        print(f"Global Model Accuracy: {global_metrics[1]:.4f}")
        print(f"Global Model Loss: {global_metrics[0]:.4f}")
        print("-" * 30)

    # Save the global model
    global_model.save(f"{MODEL_NAME}.h5")

    # Final evaluation
    final_metrics = global_model.evaluate(test_generator, verbose=0)
    print("\nFinal Test Results:")
    print("-" * 20)
    print(f"Test Loss: {final_metrics[0]:.4f}")
    print(f"Test Accuracy: {final_metrics[1]:.4f}")

    # Autotest Evaluation
    autotest_metrics = global_model.evaluate(autotest_generator,verbose=0)
    print("\nAutoTest Results:")
    print("-" * 20)
    print(f"Test Loss: {autotest_metrics[0]:.4f}")
    print(f"Test Accuracy: {autotest_metrics[1]:.4f}")

    # Print classification report
    y_true = test_generator.classes
    y_pred = np.argmax(global_model.predict(test_generator), axis=1)
    print("\nTest dataset - Classification Report:")
    print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))

    # Print autotest dataset classification report
    y_true_auto = autotest_generator.classes
    y_pred_auto = np.argmax(global_model.predict(autotest_generator),axis=1)
    print("\nAutotest dataset - Classification Report:")
    print(classification_report(y_true_auto, y_pred_auto, target_names=autotest_generator.class_indices.keys()))

    # Plot test dataset confusion matrix
    print("\nTest dataset - Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=test_generator.class_indices.keys(),
                yticklabels=test_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Test Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_test_confusion_matrix.png")
    plt.show()

    # Plot autotest dataset - confusion matrix
    print("\nAutotest dataset - Confusion Matrix:")
    conf_matrix = confusion_matrix(y_true_auto, y_pred_auto)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=autotest_generator.class_indices.keys(),
                yticklabels=autotest_generator.class_indices.keys())
    plt.title(f'{MODEL_NAME} Autotest Dataset Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(f"{MODEL_NAME}_autotest_confusion_matrix.png")
    plt.show()

    # Plot global model accuracy and loss for each round
    rounds = np.arange(1, NUM_ROUNDS + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(rounds, global_accuracies_history, marker='o', label='Global Accuracy')
    plt.title(f'{MODEL_NAME} Global Model Accuracy')
    plt.xlabel('Rounds')
    plt.ylabel('Accuracy')
    plt.xticks(rounds)
    plt.ylim([0, 1])
    plt.grid()
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(rounds, [global_metrics[0]] * NUM_ROUNDS, marker='o', label='Global Loss')
    plt.title(f'{MODEL_NAME} Global Model Loss')
    plt.xlabel('Rounds')
    plt.ylabel('Loss')
    plt.xticks(rounds)
    plt.grid()
    plt.legend()

    plt.tight_layout()
    plt.savefig(f"{MODEL_NAME}_global_accuracy_loss.png")
    plt.show()

    # Save accuracies DataFrame to a CSV file
    accuracies_df.to_csv(f"{MODEL_NAME}_accuracies.csv", index=False)

if __name__ == "__main__":
    main()

from google.colab import files

# List of files to download
file_names = [
    "/content/InceptionResNet.h5",
    "/content/InceptionResNet_accuracies.csv",
    "/content/InceptionResNet_autotest_confusion_matrix.png",
    "/content/InceptionResNet_global_accuracy_loss.png",
    "/content/InceptionResNet_test_confusion_matrix.png",
    "/content/DenseNet201.h5"
]

# Download each file
for file_name in file_names:
    files.download(file_name)

